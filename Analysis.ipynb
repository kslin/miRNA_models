{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow on RNA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset object that holds features and labels and cycles through the data in batches\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        assert (len(features) == len(labels))\n",
    "        self.features = np.array(features)\n",
    "        self.labels = np.array(labels)\n",
    "        self.index = 0\n",
    "        self.size = len(labels)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        old_index = self.index\n",
    "        new_index = self.index + batch_size\n",
    "        self.index = new_index % self.size\n",
    "        if new_index <= self.size:\n",
    "            return (self.features[old_index: new_index], self.labels[old_index: new_index])\n",
    "        else:\n",
    "            subfeatures = np.concatenate([self.features[old_index:], self.features[:self.index]])\n",
    "            sublabels = np.concatenate([self.labels[old_index:], self.labels[:self.index]])\n",
    "            return (subfeatures, sublabels)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.index = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import sample data for digit recognition\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_train = Dataset(mnist.train.images, mnist.train.labels)\n",
    "mnist_test = Dataset(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def train_model(sess, train_step, eval_var, num_epoch, batch_size, report_int, keep_prob_train, train, test):\n",
    "\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train epochs\n",
    "    for i in range(num_epoch):\n",
    "        batch = train.next_batch(50)\n",
    "        if i%report_int == 0:\n",
    "            train_accuracy = eval_var.eval(feed_dict={x:batch[0],\n",
    "                                                      y_: batch[1],\n",
    "                                                      keep_prob: 1.0})\n",
    "\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy),\n",
    "                  \"test accuracy %g\"%eval_var.eval(feed_dict={x: test.features,\n",
    "                                                              y_: test.labels,\n",
    "                                                              keep_prob: 1.0}))\n",
    "        train_step.run(feed_dict={x: batch[0],\n",
    "                                  y_: batch[1],\n",
    "                                  keep_prob: keep_prob_train})\n",
    "\n",
    "    print(\"test accuracy %g\"%eval_var.eval(feed_dict={x: test.features,\n",
    "                                                      y_: test.labels,\n",
    "                                                      keep_prob: 1.0}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First implement MNIST with a convolutional NN using tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_size = 28\n",
    "final_size = 7\n",
    "num_output1 = 32\n",
    "num_output2 = 64\n",
    "fully_connected_nodes = 1024\n",
    "out_nodes = 10\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, init_size*init_size])\n",
    "x_image = tf.reshape(x, [-1,init_size,init_size,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([final_size * final_size * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, final_size * final_size * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08 test accuracy 0.0731\n",
      "step 100, training accuracy 0.66 test accuracy 0.7809\n",
      "step 200, training accuracy 0.9 test accuracy 0.8655\n",
      "step 300, training accuracy 0.92 test accuracy 0.9016\n",
      "step 400, training accuracy 0.9 test accuracy 0.915\n",
      "step 500, training accuracy 0.94 test accuracy 0.9169\n",
      "step 600, training accuracy 0.94 test accuracy 0.9289\n",
      "step 700, training accuracy 1 test accuracy 0.9345\n",
      "step 800, training accuracy 0.92 test accuracy 0.9444\n",
      "step 900, training accuracy 0.98 test accuracy 0.9393\n",
      "test accuracy 0.9464\n"
     ]
    }
   ],
   "source": [
    "# use a cross entropy loss function and optimize with Adam\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_model(sess, train_step, accuracy, 1000, 50, 100, mnist_train, mnist_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classify RNA sequences based on complementarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_sequence(length):\n",
    "    \"\"\"Generate a random RNA sequence of a given length\"\"\"\n",
    "    \n",
    "    nts = ['A','U','C','G']\n",
    "    sequence = np.random.choice(nts, size=length, replace=True)\n",
    "\n",
    "    return ''.join(sequence)\n",
    "\n",
    "def get_complementary(seq):\n",
    "    \"\"\"Get the complementary sequence of a given RNA sequence\"\"\"\n",
    "    \n",
    "    intab = \"AUCG\"\n",
    "    outtab = \"UAGC\"\n",
    "    trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "    return seq.translate(trantab)\n",
    "\n",
    "def generate_match_pair(length, random_seed=None):\n",
    "    \"\"\"Generate two sequences that are base-paired\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    seq1 = generate_random_sequence(length)\n",
    "    seq2 = get_complementary(seq1)\n",
    "    \n",
    "    return seq1, seq2\n",
    "\n",
    "def generate_seed_match_pair(length1, length2, random_seed=None):\n",
    "    \"\"\"Generate two sequences that are base-paired at positions 1-7\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    seq1 = generate_random_sequence(length1)\n",
    "    up_fragment = generate_random_sequence(1)\n",
    "    down_fragment = generate_random_sequence(length2-7)\n",
    "    mid_fragment = get_complementary(seq1[1:7])\n",
    "    \n",
    "    seq2 = up_fragment + mid_fragment + down_fragment\n",
    "    \n",
    "    return seq1, seq2\n",
    "\n",
    "def generate_random_pair(length1, length2, random_seed=None):\n",
    "    \"\"\"Generate two random sequences that are not perfectly complementary\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    seq1 = generate_random_sequence(length1)\n",
    "    match_seq1 = get_complementary(seq1)\n",
    "\n",
    "    while True:\n",
    "        seq2 = generate_random_sequence(length2)\n",
    "\n",
    "        if match_seq1 != seq2:\n",
    "            return seq1, seq2\n",
    "\n",
    "def one_hot_encode(seq, nt_order):\n",
    "    \"\"\"Convert RNA sequence to one-hot encoding\"\"\"\n",
    "    \n",
    "    one_hot = [list(np.array(nt_order == nt, dtype=int)) for nt in seq]\n",
    "    one_hot = [item for sublist in one_hot for item in sublist]\n",
    "    \n",
    "    return np.array(one_hot)\n",
    "\n",
    "def make_square(seq1, seq2):\n",
    "    \"\"\"Given two sequences, calculate outer product of one-hot encodings\"\"\"\n",
    "\n",
    "    return np.outer(one_hot_encode(seq1, np.array(['A','U','C','G'])),\n",
    "                    one_hot_encode(seq2, np.array(['U','A','G','C'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CUAGCUCCUGAGGACUACUC', 'AAUCGAGCAGUCUCGAUAGU')\n",
      "('CAUCCUAUAGCCUACUCGGC', 'GCAACGUCGAAUCUUGGAGG')\n",
      "('GCGGACUAGCGCACAUCCGG', 'CGCCUGAUCGCGUGUAGGCC')\n",
      "[[0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(generate_seed_match_pair(20,20))\n",
    "print(generate_random_pair(20,20))\n",
    "print(generate_match_pair(20,20))\n",
    "print(make_square('AAA','AAA'))\n",
    "print(make_square('UAG','AUC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "# make 2D neural network object\n",
    "class NeuralNet2D(object):\n",
    "    \n",
    "    def __init__(self, sess, dim1, dim2, label_size):\n",
    "        \"\"\"Initiate object with placeholders for data and layers\"\"\"\n",
    "        self.sess = sess\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, dim1*dim2])\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[None, label_size])\n",
    "        \n",
    "        print(self.x.get_shape())\n",
    "        \n",
    "        self.x_image = tf.summary.image('images', tf.reshape(self.x, [-1, dim1, dim2, 1]))\n",
    "        \n",
    "        self.layers = [tf.reshape(self.x, [-1,dim1,dim2,1])]\n",
    "        self.layer_index = 0\n",
    "    \n",
    "    def add_layer(self, input_tensor, weight_dim, output_dim, layer_name, preact, act):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "\n",
    "        It does a matrix multiply, bias add, and then uses an activation function to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # add a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # create variables for weights and biases\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable(weight_dim)\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_dim])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                preactivate = preact(input_tensor, weights, biases)\n",
    "                tf.summary.image('pre_activations', preactivate)\n",
    "            \n",
    "            out_layer = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', out_layer)\n",
    "            \n",
    "            return out_layer\n",
    "    \n",
    "    def add_convolution(self, layer_name, dim1, dim2, stride1, stride2,\n",
    "                        output_channels, padding='SAME', act=tf.nn.relu):\n",
    "        \"\"\"Create a convolution layer. \n",
    "        Dim1 and dim2 specify the size of the box for the convolution\n",
    "        \"\"\"\n",
    "        # get the current layer and determine how many output channels it has\n",
    "        current_layer = self.layers[self.layer_index]\n",
    "\n",
    "        # this becomes the new input channel size\n",
    "        input_channels = current_layer.get_shape().as_list()[-1]\n",
    "        \n",
    "        # specify the function that creates a new tensor\n",
    "        preact = lambda tensor, weights, biases: (tf.nn.conv2d(tensor,\n",
    "                                                               weights,\n",
    "                                                               strides=[1, stride1, stride2, 1],\n",
    "                                                               padding=padding) + biases)\n",
    "\n",
    "        # create the new tensor\n",
    "        new_layer = self.add_layer(current_layer,\n",
    "                                   [dim1, dim2, input_channels, output_channels],\n",
    "                                   output_channels, layer_name, preact, act)\n",
    "\n",
    "        self.layers.append(new_layer)\n",
    "        self.layer_index += 1\n",
    "\n",
    "    def add_max_pool(self, dim1, dim2, stride1, stride2, padding='SAME'):\n",
    "        \"\"\"Adds a max pooling layer.\"\"\"\n",
    "        self.layers.append(tf.nn.max_pool(self.layers[self.layer_index],\n",
    "                                            ksize=[1, dim1, dim2, 1],\n",
    "                                            strides=[1, stride1, stride2, 1], padding=padding))\n",
    "        \n",
    "        self.layer_index += 1\n",
    "\n",
    "    def add_fully_connected(self, layer_name, output_channels, act=tf.nn.relu):\n",
    "        \"\"\"Adds a fully connected layer.\"\"\"\n",
    "\n",
    "        current_layer = self.layers[self.layer_index]\n",
    "        dim = current_layer.get_shape().as_list()\n",
    "        dim = dim[1] * dim[2] * dim[3]\n",
    "        \n",
    "        preact = lambda tensor, weights, biases: tf.matmul(tensor, weights) + biases\n",
    "    \n",
    "        new_layer = self.add_layer(tf.reshape(current_layer, [-1, dim]),\n",
    "                                   [dim, output_channels],\n",
    "                                   output_channels, layer_name, preact, act)\n",
    "            \n",
    "        self.layers.append(new_layer)\n",
    "        self.layer_index += 1\n",
    "    \n",
    "    def add_dropout(self, layer_name, num_nodes):\n",
    "        \"\"\"Adds a layer for dropout nodes in order to reduce overfitting\"\"\"\n",
    "        current_layer = self.layers[self.layer_index]\n",
    "        dim = current_layer.get_shape().as_list()\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        with tf.name_scope(layer_name):\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([dim[-1], num_nodes])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([num_nodes])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                out_layer = tf.matmul(tf.nn.dropout(current_layer, self.keep_prob), weights) + biases\n",
    "\n",
    "            tf.summary.histogram('activations', out_layer)\n",
    "\n",
    "        self.layers.append(out_layer)\n",
    "        self.layer_index += 1\n",
    "    \n",
    "    def make_train_step(self, problem_type, logdir):\n",
    "        current_layer = self.layers[self.layer_index]\n",
    "        if problem_type == 'classification':\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(current_layer,\n",
    "                                                                                   self.y_))\n",
    "            with tf.name_scope('train'):\n",
    "                self.train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(current_layer,1), tf.argmax(self.y_,1))\n",
    "            \n",
    "            with tf.name_scope('accuracy'):\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "        elif problem_type == 'regression':\n",
    "            SS_err = tf.reduce_sum(tf.square(tf.sub(current_layer, self.y_)))\n",
    "            SS_tot = tf.reduce_sum(tf.square(tf.sub(self.y_, tf.reduce_mean(self.y_))))\n",
    "            R_2 = tf.sub(tf.cast(1.0, tf.float32), tf.div(SS_err, SS_tot))\n",
    "\n",
    "            with tf.name_scope('train'):\n",
    "                self.train_step = tf.train.AdamOptimizer(1e-4).minimize(SS_err)\n",
    "            with tf.name_scope('accuracy'):\n",
    "                self.accuracy = R_2\n",
    "        \n",
    "        else:\n",
    "            print('problem_type must be \\'classification\\' or \\'regression\\'')\n",
    "            \n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(logdir + '/train', self.sess.graph)\n",
    "        self.test_writer = tf.summary.FileWriter(logdir + '/test')\n",
    "    \n",
    "    def train_model(self, train, test, num_epoch=20000, batch_size=50, \n",
    "                    report_int=1000, keep_prob_train=0.5):\n",
    "\n",
    "        # initialize variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "\n",
    "        # train epochs\n",
    "        for i in range(num_epoch):\n",
    "            batch = train.next_batch(batch_size)\n",
    "            if i%report_int == 0:\n",
    "                \n",
    "                acc, summary = self.sess.run([self.accuracy, self.merged], feed_dict={self.x: test.features,\n",
    "                                                                                 self.y_: test.labels,\n",
    "                                                                                 self.keep_prob: 1.0})\n",
    "                self.test_writer.add_summary(summary, i)\n",
    "#                 print('Accuracy at step %s: %s' % (i, acc))\n",
    "            \n",
    "            else:\n",
    "                _, summary = self.sess.run([self.train_step, self.merged], feed_dict={self.x: batch[0],\n",
    "                                                                                 self.y_: batch[1],\n",
    "                                                                                 self.keep_prob: keep_prob_train})\n",
    "                self.train_writer.add_summary(summary, i)\n",
    "\n",
    "        print(\"test accuracy %g\"%self.accuracy.eval(feed_dict={self.x: test.features,\n",
    "                                                               self.y_: test.labels,\n",
    "                                                               self.keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to distinguish complementary sequences from random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate complementary and random 4mers\n",
    "features = np.zeros((1000, 256))\n",
    "labels = np.zeros((1000, 2))\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_match_pair(4)\n",
    "        labels[i,:] = [1, 0]\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(4,4)\n",
    "        labels[i,:] = [0, 1]\n",
    "\n",
    "    features[i,:] = make_square(seq1, seq2).flatten()\n",
    "\n",
    "match_train = Dataset(features[:900, :], labels[:900, :])\n",
    "match_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 256)\n",
      "test accuracy 0.97\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "    dim1, dim2 = 16, 16\n",
    "    num_output1 = 4\n",
    "    fully_connected_nodes = 1024\n",
    "    out_nodes = 2\n",
    "\n",
    "    NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "    NN.add_convolution('layer1', 4, 4, 4, 4, num_output1)\n",
    "    NN.add_fully_connected('layer2', fully_connected_nodes)\n",
    "    NN.add_dropout('layer3', out_nodes)\n",
    "    NN.make_train_step('classification', '../logdirs/match')\n",
    "    NN.train_model(match_train, match_test, num_epoch=1000,\n",
    "                   batch_size=50, report_int=10, keep_prob_train=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to distinguish seed-matched sequences from random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate mirna,utr pairs of length 20 with a seed match\n",
    "features = np.zeros((1000, 6400))\n",
    "labels = np.zeros((1000,2))\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_seed_match_pair(20,20)\n",
    "        labels[i,:] = [1, 0]\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(20,20)\n",
    "        labels[i,:] = [0, 1]\n",
    "    \n",
    "    features[i,:] = make_square(seq1, seq2).flatten()\n",
    "\n",
    "seed_match_train = Dataset(features[:900, :], labels[:900, :])\n",
    "seed_match_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add_convolution() missing 1 required positional argument: 'output_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-115b994d761a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_convolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_fully_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfully_connected_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add_convolution() missing 1 required positional argument: 'output_channels'"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 80, 80\n",
    "num_output1 = 4\n",
    "fully_connected_nodes = 1024\n",
    "out_nodes = 2\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('classification')\n",
    "NN.train_model(seed_match_train, seed_match_test, num_epoch=5000,\n",
    "               batch_size=50, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to regress on seed pairing stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_sps(seq):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ==========\n",
    "    seq: string, consist of A, U, C, G\n",
    "    \n",
    "    Returns:\n",
    "    =======\n",
    "    float: seed pairing stability\n",
    "    \"\"\"\n",
    "    thermo_dict = {'AA':-0.93,'AU':-1.10,'AC':-2.24,'AG':-2.08,\n",
    "               'UA':-1.33,'UU':-0.93,'UC':-2.35,'UG':-2.11,\n",
    "               'CA':-2.11,'CU':-2.08,'CC':-3.26,'CG':-2.36,\n",
    "               'GA':-2.35,'GU':-2.24,'GC':-3.42,'GG':-3.26}\n",
    "    init = 4.09\n",
    "    terminal_au = 0.45\n",
    "    \n",
    "    # initialize score\n",
    "    score = init\n",
    "    \n",
    "    # add score for each dinucleotide\n",
    "    for i in range(len(seq)-1):\n",
    "        score += thermo_dict[seq[i:i+2]]\n",
    "    \n",
    "    # add score for each terminal AU\n",
    "    score += terminal_au*((seq[0]+seq[-1]).count('A') + (seq[0]+seq[-1]).count('U'))\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random sequences\n",
    "features = np.zeros((1000, 6400))\n",
    "labels = np.zeros((1000,1))\n",
    "seq1s, seq2s = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_seed_match_pair(20,20)\n",
    "        seq1s.append(seq1)\n",
    "        seq2s.append(seq2)\n",
    "        labels[i,:] = calculate_sps(seq1[1:7]) + (np.random.random()-0.5)\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(20,20)\n",
    "        seq1s.append(seq1)\n",
    "        seq2s.append(seq2)\n",
    "        labels[i,:] = np.random.random()-0.5\n",
    "    \n",
    "    features[i,:] = make_square(seq1, seq2).flatten()\n",
    "\n",
    "seed_match_sps_train = Dataset(features[:900, :], labels[:900, :])\n",
    "seed_match_sps_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy -1.04116 test accuracy -0.901628\n",
      "step 1000, training accuracy 0.540802 test accuracy 0.367427\n",
      "step 2000, training accuracy 0.942358 test accuracy 0.861274\n",
      "step 3000, training accuracy 0.981681 test accuracy 0.913068\n",
      "step 4000, training accuracy 0.988818 test accuracy 0.926939\n",
      "step 5000, training accuracy 0.987021 test accuracy 0.925711\n",
      "step 6000, training accuracy 0.985119 test accuracy 0.928617\n",
      "step 7000, training accuracy 0.99206 test accuracy 0.936477\n",
      "step 8000, training accuracy 0.996499 test accuracy 0.941244\n",
      "step 9000, training accuracy 0.994621 test accuracy 0.942679\n",
      "test accuracy 0.944907\n"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 80, 80\n",
    "num_output1 = 16\n",
    "num_output2 = 16\n",
    "fully_connected_nodes = 32\n",
    "out_nodes = 1\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_convolution(2, 2, 1, 1, num_output2)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('regression')\n",
    "NN.train_model(seed_match_sps_train, seed_match_sps_test, num_epoch=10000,\n",
    "               batch_size=100, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regression on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCCGGUAAU'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LOGFC_FILE1 = '../data/Supplementary1.csv'\n",
    "LOGFC_FILE2 = '../data/Supplementary2.csv'\n",
    "GENE_FILE = '../data/Gene_info.txt'\n",
    "SEED_FILE = '../data/seed_dict.csv'\n",
    "SEQ_FILE = '../data/seq_dict.csv'\n",
    "\n",
    "def rev_comp(seq):\n",
    "    \"\"\"Get the reverse complement of a given RNA sequence\"\"\"\n",
    "    \n",
    "    intab = \"AUCG\"\n",
    "    outtab = \"UAGC\"\n",
    "    trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "    return seq.translate(trantab)[::-1]\n",
    "    \n",
    "rev_comp('AUUACCGGC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>mirna_seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCGUAGG</th>\n",
       "      <td>1595297366</td>\n",
       "      <td>UUCGUAGGUCAAAAUACACAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCAUCUC</th>\n",
       "      <td>1595297383</td>\n",
       "      <td>UUCAUCUCCAAUUCGUAGGAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UGCUCUU</th>\n",
       "      <td>1595297389</td>\n",
       "      <td>AUGCUCUUUCCUCCUGUGCAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UUUGGAA</th>\n",
       "      <td>1595297394</td>\n",
       "      <td>UUUUGGAACAGUCUUUCCGAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UUGGAAC</th>\n",
       "      <td>1595297399</td>\n",
       "      <td>UUUGGAACAGUCUUUCCGAAAAAA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                col                 mirna_seq\n",
       "seed                                         \n",
       "UCGUAGG  1595297366  UUCGUAGGUCAAAAUACACAAAAA\n",
       "UCAUCUC  1595297383  UUCAUCUCCAAUUCGUAGGAAAAA\n",
       "UGCUCUU  1595297389  AUGCUCUUUCCUCCUGUGCAAAAA\n",
       "UUUGGAA  1595297394  UUUUGGAACAGUCUUUCCGAAAAA\n",
       "UUGGAAC  1595297399  UUUGGAACAGUCUUUCCGAAAAAA"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENE_INFO = pd.read_csv(GENE_FILE,sep='\\t').drop(['Gene description','Species ID'],1)\n",
    "GENE_INFO = GENE_INFO.groupby('Gene symbol').agg(lambda x:tuple(x))\n",
    "GENE_INFO.loc[:,'Isoform ratio'] = [float(max(x))/np.nansum(x) for x in GENE_INFO['3P-seq tags + 5']]\n",
    "GENE_INFO.loc[:,'Transcript ID'] = [x[y.index(1)] for (x,y) in zip(GENE_INFO['Transcript ID'],GENE_INFO['Representative transcript?'])]\n",
    "GENE_INFO = GENE_INFO[['Transcript ID','Isoform ratio']]\n",
    "GENE_INFO_TRANSCRIPT = GENE_INFO.reset_index().set_index('Transcript ID')\n",
    "\n",
    "SEED_INFO = pd.read_csv(SEED_FILE,sep='\\t')\n",
    "SEED_DICT = {}\n",
    "for row in SEED_INFO.iterrows():\n",
    "    SEED_DICT[row[1]['col']] = row[1]['seed']\n",
    "\n",
    "SEEDS = sorted(list(SEED_DICT.values()))\n",
    "\n",
    "SEQ_INFO = pd.read_csv(SEQ_FILE, sep='\\t')\n",
    "SEQ_INFO['seed'] = [SEED_DICT[x] for x in SEQ_INFO['col']]\n",
    "SEQ_INFO = SEQ_INFO.set_index('seed')\n",
    "SEQ_INFO['mirna_seq'] = [(x + 'AAAAAAA')[:24] for x in SEQ_INFO['mirna_seq']]\n",
    "SEQ_INFO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19432\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>query</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NM_001003806</th>\n",
       "      <td>TARP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM_001003805</th>\n",
       "      <td>ATP5S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM_001003802</th>\n",
       "      <td>SMARCD3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM_001003803</th>\n",
       "      <td>ATP5S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NM_001003800</th>\n",
       "      <td>BICD2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               symbol\n",
       "query                \n",
       "NM_001003806     TARP\n",
       "NM_001003805    ATP5S\n",
       "NM_001003802  SMARCD3\n",
       "NM_001003803    ATP5S\n",
       "NM_001003800    BICD2"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refseq_gene = pd.read_csv('../../05_TargetPrediction/data/refseq_gene.txt',sep='\\t').set_index('query')\n",
    "print(len(GENE_INFO))\n",
    "refseq_gene.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Transcript ID</th>\n",
       "      <th>Isoform ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gene symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENST00000401030.3</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1BG</th>\n",
       "      <td>ENST00000263100.3</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1CF</th>\n",
       "      <td>ENST00000374001.2</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2M</th>\n",
       "      <td>ENST00000318602.7</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2ML1</th>\n",
       "      <td>ENST00000299698.7</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Transcript ID  Isoform ratio\n",
       "Gene symbol                                  \n",
       "0            ENST00000401030.3       0.333333\n",
       "A1BG         ENST00000263100.3       1.000000\n",
       "A1CF         ENST00000374001.2       1.000000\n",
       "A2M          ENST00000318602.7       1.000000\n",
       "A2ML1        ENST00000299698.7       1.000000"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENE_INFO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35956\n",
      "22351\n",
      "18200\n",
      "18200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Average expression</th>\n",
       "      <th>Isoform ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Transcript ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000000233.5</th>\n",
       "      <td>45.384213</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000412.3</th>\n",
       "      <td>27.196628</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001008.4</th>\n",
       "      <td>35.387227</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001146.2</th>\n",
       "      <td>1.151177</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000002125.4</th>\n",
       "      <td>4.902705</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Average expression  Isoform ratio\n",
       "Transcript ID                                       \n",
       "ENST00000000233.5           45.384213            1.0\n",
       "ENST00000000412.3           27.196628            1.0\n",
       "ENST00000001008.4           35.387227            1.0\n",
       "ENST00000001146.2            1.151177            1.0\n",
       "ENST00000002125.4            4.902705            1.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression = pd.read_csv('../../05_TargetPrediction/data/GSE52530_HeLa.expData.qn.txt.gz',sep='\\t')[['#geneID','geneLen','Puc19_1,Puc19_2']]\n",
    "expression['Average expression'] = [np.mean([float(x) for x in exp.split(',')]) for exp in list(expression['Puc19_1,Puc19_2'])]\n",
    "# expression['RPM'] = (expression['Average expression'] * expression['geneLen'])/ 1000.0\n",
    "expression = expression.drop(['geneLen','Puc19_1,Puc19_2'],1)\n",
    "\n",
    "expression['Gene symbol'] = list(refseq_gene.loc[expression['#geneID']]['symbol'])\n",
    "print(len(expression))\n",
    "expression = expression.dropna().groupby('Gene symbol').agg(np.nanmean)\n",
    "print(len(expression))\n",
    "expression = pd.concat([expression, GENE_INFO], axis=1, join='inner')\n",
    "print(len(expression))\n",
    "expression = expression.reset_index().groupby('Transcript ID').first().drop('Gene symbol',1)\n",
    "print(len(expression))\n",
    "expression.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.0205855781509987"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(expression['Average expression'], 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTR sequence</th>\n",
       "      <th>UTR length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensembl ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CDR1as</th>\n",
       "      <td>GUUUCCGAUGGCACCUGUGUCAAGGUCUUCCAACAACUCCGGGUCU...</td>\n",
       "      <td>1485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000233.5</th>\n",
       "      <td>CCAGCCAGGGGCAGGCCCCUGAUGCCCGGAAGCUCCUGCGUGCAUC...</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000412.3</th>\n",
       "      <td>AUUGCACUUUAUAUGUCCAGCCUCUUCCUCAGUCCCCCAAACCAAA...</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001008.4</th>\n",
       "      <td>CCCCUCUCCACCAGCCCUACUCCUGCGGCUGCCUGCCCCCCAGUCU...</td>\n",
       "      <td>2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001146.2</th>\n",
       "      <td>CCCAAGACCCACCCGCCUCAGCCCAGCCCAGGCAGCGGGGUGGUGG...</td>\n",
       "      <td>3001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        UTR sequence  \\\n",
       "Ensembl ID                                                             \n",
       "CDR1as             GUUUCCGAUGGCACCUGUGUCAAGGUCUUCCAACAACUCCGGGUCU...   \n",
       "ENST00000000233.5  CCAGCCAGGGGCAGGCCCCUGAUGCCCGGAAGCUCCUGCGUGCAUC...   \n",
       "ENST00000000412.3  AUUGCACUUUAUAUGUCCAGCCUCUUCCUCAGUCCCCCAAACCAAA...   \n",
       "ENST00000001008.4  CCCCUCUCCACCAGCCCUACUCCUGCGGCUGCCUGCCCCCCAGUCU...   \n",
       "ENST00000001146.2  CCCAAGACCCACCCGCCUCAGCCCAGCCCAGGCAGCGGGGUGGUGG...   \n",
       "\n",
       "                   UTR length  \n",
       "Ensembl ID                     \n",
       "CDR1as                   1485  \n",
       "ENST00000000233.5         422  \n",
       "ENST00000000412.3        1457  \n",
       "ENST00000001008.4        2163  \n",
       "ENST00000001146.2        3001  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTRS = pd.read_csv('../../05_TargetPrediction/targetscan_files/UTR_Sequences_Ensembl_Human.txt',\n",
    "                   sep='\\t', usecols=['Ensembl ID','UTR sequence']).set_index('Ensembl ID')\n",
    "UTRS['UTR sequence'] = [x.replace('-','').upper().replace('T','U') for x in UTRS['UTR sequence']]\n",
    "UTRS['UTR length'] = [len(x) for x in UTRS['UTR sequence']]\n",
    "# UTRS = pd.concat([UTRS, expression], axis=1, join='inner')\n",
    "UTRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UCGUAGG</th>\n",
       "      <th>UCAUCUC</th>\n",
       "      <th>UGCUCUU</th>\n",
       "      <th>UUUGGAA</th>\n",
       "      <th>UUGGAAC</th>\n",
       "      <th>CAAACAC</th>\n",
       "      <th>AAUACAC</th>\n",
       "      <th>UUUCCUC</th>\n",
       "      <th>AGCUUCC</th>\n",
       "      <th>AGUCAGA</th>\n",
       "      <th>...</th>\n",
       "      <th>AGCAGCA</th>\n",
       "      <th>AAAGUGC</th>\n",
       "      <th>AACACUG</th>\n",
       "      <th>AAUACUG</th>\n",
       "      <th>UGACCUA</th>\n",
       "      <th>GAGGUAG</th>\n",
       "      <th>GCAGCAU</th>\n",
       "      <th>Average expression</th>\n",
       "      <th>Isoform ratio</th>\n",
       "      <th>UTR length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000000233.5</th>\n",
       "      <td>0.069</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.065</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45.384213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000412.3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.078</td>\n",
       "      <td>27.196628</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001008.4</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>35.387227</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000002165.6</th>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>24.530939</td>\n",
       "      <td>0.999025</td>\n",
       "      <td>1774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000002829.3</th>\n",
       "      <td>0.461</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.168</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.209</td>\n",
       "      <td>0.182</td>\n",
       "      <td>-0.088</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.205</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.912061</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 84 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UCGUAGG  UCAUCUC  UGCUCUU  UUUGGAA  UUGGAAC  CAAACAC  \\\n",
       "ENST00000000233.5    0.069    0.018    0.048   -0.033   -0.076   -0.007   \n",
       "ENST00000000412.3      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000001008.4   -0.016   -0.557    0.110    0.241   -0.231    0.054   \n",
       "ENST00000002165.6   -0.058    0.092    0.036    0.038   -0.191    0.067   \n",
       "ENST00000002829.3    0.461    0.208    0.725    0.168    0.003   -0.209   \n",
       "\n",
       "                   AAUACAC  UUUCCUC  AGCUUCC  AGUCAGA     ...      AGCAGCA  \\\n",
       "ENST00000000233.5    0.059    0.055   -0.019    0.065     ...          NaN   \n",
       "ENST00000000412.3      NaN      NaN      NaN      NaN     ...       -0.098   \n",
       "ENST00000001008.4   -0.031    0.160   -0.021    0.099     ...        0.016   \n",
       "ENST00000002165.6    0.057    0.046   -0.055   -0.075     ...        0.063   \n",
       "ENST00000002829.3    0.182   -0.088    0.063    0.112     ...       -0.205   \n",
       "\n",
       "                   AAAGUGC  AACACUG  AAUACUG  UGACCUA  GAGGUAG  GCAGCAU  \\\n",
       "ENST00000000233.5      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000000412.3   -0.514   -0.056    0.031   -0.003    0.039    0.078   \n",
       "ENST00000001008.4    0.133    0.007   -0.114   -0.166    0.002    0.038   \n",
       "ENST00000002165.6    0.024   -0.018    0.178    0.015    0.031   -0.176   \n",
       "ENST00000002829.3    0.032    0.146   -0.240    0.064    0.014      NaN   \n",
       "\n",
       "                   Average expression  Isoform ratio  UTR length  \n",
       "ENST00000000233.5           45.384213       1.000000         422  \n",
       "ENST00000000412.3           27.196628       1.000000        1457  \n",
       "ENST00000001008.4           35.387227       1.000000        2163  \n",
       "ENST00000002165.6           24.530939       0.999025        1774  \n",
       "ENST00000002829.3           23.912061       1.000000         960  \n",
       "\n",
       "[5 rows x 84 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFCs1 = pd.read_csv(LOGFC_FILE1)\n",
    "# logFCs1 = logFCs1[logFCs1['Used in training'] == 'yes']\n",
    "logFCs1['Transcript ID'] = list(GENE_INFO.loc[logFCs1['Gene symbol']]['Transcript ID'])\n",
    "logFCs1 = logFCs1.drop(['Used in training','RefSeq ID','Gene symbol'],1).dropna(subset=['Transcript ID'])\n",
    "logFCs1 = logFCs1.drop_duplicates(subset=['Transcript ID']).set_index('Transcript ID')\n",
    "logFCs1.columns = [SEED_DICT[x] if x in SEED_DICT else x for x in logFCs1.columns]\n",
    "\n",
    "logFCs2 = pd.read_csv(LOGFC_FILE2)\n",
    "logFCs2['Transcript ID'] = list(GENE_INFO.loc[logFCs2['Gene symbol']]['Transcript ID'])\n",
    "logFCs2 = logFCs2.drop(['RefSeq ID','Gene symbol'],1).dropna(subset=['Transcript ID'])\n",
    "logFCs2 = logFCs2.drop_duplicates(subset=['Transcript ID']).set_index('Transcript ID')\n",
    "logFCs2.columns = [SEED_DICT[x] if x in SEED_DICT else x for x in logFCs2.columns]\n",
    "\n",
    "logFCs = pd.concat([logFCs1,logFCs2],axis=1,join='outer')\n",
    "# logFCs = pd.concat([logFCs, GENE_INFO_TRANSCRIPT], axis=1, join='inner')\n",
    "logFCs = pd.concat([logFCs, expression, UTRS[['UTR length']]], axis=1, join='inner')\n",
    "\n",
    "logFCs = logFCs[logFCs['Isoform ratio'] > 0.9]\n",
    "logFCs = logFCs[logFCs['Average expression'] > np.percentile(expression['Average expression'], 50)]\n",
    "print(len(logFCs))\n",
    "logFCs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datasets with site type and SPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42837, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipped = list(zip(SEEDS, [rev_comp(seed) for seed in SEEDS], [SEQ_INFO.loc[seed]['mirna_seq'] for seed in SEEDS]))\n",
    "features, labels, sequences = [], [], []\n",
    "\n",
    "for row in logFCs.iterrows():\n",
    "    utr = UTRS.loc[row[0]]['UTR sequence']\n",
    "    row = row[1]\n",
    "    for seed, rev, mirna in zipped:\n",
    "        val = row[seed]\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "        elif utr.count(rev[1:]) == 1:\n",
    "            loc = utr.find(rev[1:])\n",
    "            if (loc-15) >= 0:\n",
    "                if (loc + 9) < len(utr):\n",
    "                    seq1, seq2 = mirna[:20], utr[loc-15:loc+9]\n",
    "                    sequences.append((seq1, seq2[::-1]))\n",
    "                    features.append(make_square(seq1, seq2[::-1]).flatten())\n",
    "                    if (rev + 'A') in seq2:\n",
    "                        labels.append(-3 + (calculate_sps(rev)/10.0))\n",
    "                    elif (rev) in seq2:\n",
    "                        labels.append(-2 + (calculate_sps(rev)/10.0))\n",
    "                    elif (rev[1:] + 'A') in seq2:\n",
    "                        labels.append(-2 + (calculate_sps(rev[1:])/10.0))\n",
    "                    elif (rev[1:]) in seq2:\n",
    "                        labels.append(-1)\n",
    "                    else:\n",
    "                        print('blah', seq2, rev)\n",
    "                        break\n",
    "\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels).reshape(len(labels), 1)\n",
    "test_size = int(len(features)/10)\n",
    "constructed_logfc_train = Dataset(features[test_size:], labels[test_size:])\n",
    "constructed_logfc_test = Dataset(features[:test_size], labels[:test_size])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42837, 7680)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/constructed_logfc_100.txt', 'w') as f:\n",
    "    i = 0\n",
    "    for seqs, label in zip(sequences, labels):\n",
    "        f.write('{},{},{}\\n'.format(seqs[0], seqs[1], label[0]))\n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy -4.88184 test accuracy -4.65476\n",
      "step 1000, training accuracy 0.402542 test accuracy 0.378784\n",
      "step 2000, training accuracy 0.509338 test accuracy 0.551205\n",
      "step 3000, training accuracy 0.845022 test accuracy 0.824984\n",
      "step 4000, training accuracy 0.928436 test accuracy 0.914872\n",
      "step 5000, training accuracy 0.9505 test accuracy 0.933533\n",
      "step 6000, training accuracy 0.948405 test accuracy 0.948905\n",
      "step 7000, training accuracy 0.963318 test accuracy 0.940776\n",
      "step 8000, training accuracy 0.953124 test accuracy 0.943543\n",
      "step 9000, training accuracy 0.920974 test accuracy 0.949701\n",
      "test accuracy 0.950582\n"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 80, 96\n",
    "num_output1 = 16\n",
    "num_output2 = 16\n",
    "fully_connected_nodes = 32\n",
    "out_nodes = 1\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_convolution(2, 2, 1, 1, num_output2)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('regression')\n",
    "NN.train_model(constructed_logfc_train, constructed_logfc_test, num_epoch=10000,\n",
    "               batch_size=100, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77423 35863 35863 35863\n"
     ]
    }
   ],
   "source": [
    "zipped = list(zip(SEEDS, [rev_comp(seed) for seed in SEEDS], [SEQ_INFO.loc[seed]['mirna_seq'] for seed in SEEDS]))\n",
    "features, labels, sequences, stypes, lens = [], [], [], [], []\n",
    "l8, l7, l71a = [], [], []\n",
    "\n",
    "i = 0\n",
    "for row in logFCs.iterrows():\n",
    "    utr = UTRS.loc[row[0]]['UTR sequence']\n",
    "    row = row[1]\n",
    "    for seed, rev, mirna in zipped:\n",
    "        val = row[seed]\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "        elif utr.count(rev[1:]) == 1:\n",
    "            loc = utr.find(rev[1:])\n",
    "            if (loc-15) >= 0:\n",
    "                if (loc + 9) < len(utr):\n",
    "                    i += 1\n",
    "                    seq1, seq2 = mirna[:20], utr[loc-15:loc+9]\n",
    "\n",
    "                    # only take better than 6mer\n",
    "                    if (rev + 'A') in seq2:\n",
    "                        stypes.append(-0.164921079958)\n",
    "                        l8.append(val)\n",
    "                    elif (rev) in seq2:\n",
    "                        stypes.append(-0.0726948723592)\n",
    "                        l7.append(val)\n",
    "                    elif (rev[1:] + 'A') in seq2:\n",
    "                        stypes.append(-0.0474966301044)\n",
    "                        l71a.append(val)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    sequences.append((seq1, seq2))\n",
    "                    labels.append(val)\n",
    "                    lens.append(row['UTR length'])\n",
    "\n",
    "print(i, len(labels), len(sequences), len(lens))\n",
    "\n",
    "with open('../data/logfc_7mer_8mer_extra.txt', 'w') as f:\n",
    "    for seqs, label, utrlen in zip(sequences, labels, lens):\n",
    "        f.write('{},{},{},{:.4}\\n'.format(seqs[0], seqs[1], label, np.log10(utrlen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy -3.45594 test accuracy -2.44972\n",
      "step 1000, training accuracy 0.000285447 test accuracy 0.00501722\n",
      "step 2000, training accuracy 0.0139347 test accuracy 0.0031606\n",
      "step 3000, training accuracy 0.0388749 test accuracy 0.0246275\n",
      "step 4000, training accuracy 0.0674052 test accuracy 0.0304471\n",
      "step 5000, training accuracy 0.0413511 test accuracy 0.0370678\n",
      "step 6000, training accuracy -0.0415183 test accuracy 0.0389867\n",
      "step 7000, training accuracy 0.0528648 test accuracy 0.0308995\n",
      "step 8000, training accuracy 0.116715 test accuracy 0.0487014\n",
      "step 9000, training accuracy 0.0626391 test accuracy 0.0537782\n",
      "test accuracy 0.0496244\n"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 80, 96\n",
    "num_output1 = 16\n",
    "num_output2 = 16\n",
    "fully_connected_nodes = 32\n",
    "out_nodes = 1\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_convolution(2, 2, 1, 1, num_output2)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('regression')\n",
    "NN.train_model(logfc_train, logfc_test, num_epoch=10000,\n",
    "               batch_size=100, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
