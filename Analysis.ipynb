{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow on RNA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open a tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset object that holds features and labels and cycles through the data in batches\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        assert (len(features) == len(labels))\n",
    "        self.features = np.array(features)\n",
    "        self.labels = np.array(labels)\n",
    "        self.index = 0\n",
    "        self.size = len(labels)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        old_index = self.index\n",
    "        new_index = self.index + batch_size\n",
    "        self.index = new_index % self.size\n",
    "        if new_index <= self.size:\n",
    "            return (self.features[old_index: new_index], self.labels[old_index: new_index])\n",
    "        else:\n",
    "            subfeatures = np.concatenate([self.features[old_index:], self.features[:self.index]])\n",
    "            sublabels = np.concatenate([self.labels[old_index:], self.labels[:self.index]])\n",
    "            return (subfeatures, sublabels)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.index = 0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import sample data for digit recognition\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_train = Dataset(mnist.train.images, mnist.train.labels)\n",
    "mnist_test = Dataset(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def train_model(sess, train_step, eval_var, num_epoch, batch_size, report_int, keep_prob_train, train, test):\n",
    "\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train epochs\n",
    "    for i in range(num_epoch):\n",
    "        batch = train.next_batch(50)\n",
    "        if i%report_int == 0:\n",
    "            train_accuracy = eval_var.eval(feed_dict={x:batch[0],\n",
    "                                                      y_: batch[1],\n",
    "                                                      keep_prob: 1.0})\n",
    "\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy),\n",
    "                  \"test accuracy %g\"%eval_var.eval(feed_dict={x: test.features,\n",
    "                                                              y_: test.labels,\n",
    "                                                              keep_prob: 1.0}))\n",
    "        train_step.run(feed_dict={x: batch[0],\n",
    "                                  y_: batch[1],\n",
    "                                  keep_prob: keep_prob_train})\n",
    "\n",
    "    print(\"test accuracy %g\"%eval_var.eval(feed_dict={x: test.features,\n",
    "                                                      y_: test.labels,\n",
    "                                                      keep_prob: 1.0}))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First implement MNIST with a convolutional NN using tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_size = 28\n",
    "final_size = 7\n",
    "num_output1 = 32\n",
    "num_output2 = 64\n",
    "fully_connected_nodes = 1024\n",
    "out_nodes = 10\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, init_size*init_size])\n",
    "x_image = tf.reshape(x, [-1,init_size,init_size,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([final_size * final_size * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, final_size * final_size * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08 test accuracy 0.0731\n",
      "step 100, training accuracy 0.66 test accuracy 0.7809\n",
      "step 200, training accuracy 0.9 test accuracy 0.8655\n",
      "step 300, training accuracy 0.92 test accuracy 0.9016\n",
      "step 400, training accuracy 0.9 test accuracy 0.915\n",
      "step 500, training accuracy 0.94 test accuracy 0.9169\n",
      "step 600, training accuracy 0.94 test accuracy 0.9289\n",
      "step 700, training accuracy 1 test accuracy 0.9345\n",
      "step 800, training accuracy 0.92 test accuracy 0.9444\n",
      "step 900, training accuracy 0.98 test accuracy 0.9393\n",
      "test accuracy 0.9464\n"
     ]
    }
   ],
   "source": [
    "# use a cross entropy loss function and optimize with Adam\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_model(sess, train_step, accuracy, 1000, 50, 100, mnist_train, mnist_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classify RNA sequences based on complementarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_sequence(length):\n",
    "    \"\"\"Generate a random RNA sequence of a given length\"\"\"\n",
    "    \n",
    "    nts = ['A','U','C','G']\n",
    "    sequence = np.random.choice(nts, size=length, replace=True)\n",
    "\n",
    "    return ''.join(sequence)\n",
    "\n",
    "def get_complementary(seq):\n",
    "    \"\"\"Get the complementary sequence of a given RNA sequence\"\"\"\n",
    "    \n",
    "    intab = \"AUCG\"\n",
    "    outtab = \"UAGC\"\n",
    "    trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "    return seq.translate(trantab)\n",
    "\n",
    "def generate_match_pair(length, random_seed=None):\n",
    "    \"\"\"Generate two sequences that are base-paired\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    seq1 = generate_random_sequence(length)\n",
    "    seq2 = get_complementary(seq1)\n",
    "    \n",
    "    return seq1, seq2\n",
    "\n",
    "def generate_seed_match_pair(length1, length2, random_seed=None):\n",
    "    \"\"\"Generate two sequences that are base-paired at positions 1-7\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    seq1 = generate_random_sequence(length1)\n",
    "    up_fragment = generate_random_sequence(1)\n",
    "    down_fragment = generate_random_sequence(length2-7)\n",
    "    mid_fragment = get_complementary(seq1[1:7])\n",
    "    \n",
    "    seq2 = up_fragment + mid_fragment + down_fragment\n",
    "    \n",
    "    return seq1, seq2\n",
    "\n",
    "def generate_random_pair(length1, length2, random_seed=None):\n",
    "    \"\"\"Generate two random sequences that are not perfectly complementary\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    seq1 = generate_random_sequence(length1)\n",
    "    match_seq1 = get_complementary(seq1)\n",
    "\n",
    "    while True:\n",
    "        seq2 = generate_random_sequence(length2)\n",
    "\n",
    "        if match_seq1 != seq2:\n",
    "            return seq1, seq2\n",
    "\n",
    "def one_hot_encode(seq, nt_order):\n",
    "    \"\"\"Convert RNA sequence to one-hot encoding\"\"\"\n",
    "    \n",
    "    one_hot = [list(np.array(nt_order == nt, dtype=int)) for nt in seq]\n",
    "    one_hot = [item for sublist in one_hot for item in sublist]\n",
    "    \n",
    "    return np.array(one_hot)\n",
    "\n",
    "def make_square(seq1, seq2):\n",
    "    \"\"\"Given two sequences, calculate outer product of one-hot encodings\"\"\"\n",
    "\n",
    "    return np.outer(one_hot_encode(seq1, np.array(['A','U','C','G'])),\n",
    "                    one_hot_encode(seq2, np.array(['U','A','G','C'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('UUACAAAGACCAACGACUUC', 'AAUGUUUGUAUUUCCAUAAC')\n",
      "('CCUAACAGGAGCUCGACUAU', 'AAUUGCUAAACCCUCCGCCC')\n",
      "('GCGGACUAGCGCACAUCCGG', 'CGCCUGAUCGCGUGUAGGCC')\n",
      "[[0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(generate_seed_match_pair(20,20))\n",
    "print(generate_random_pair(20,20))\n",
    "print(generate_match_pair(20,20))\n",
    "print(make_square('AAA','AAA'))\n",
    "print(make_square('UAG','AUC'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make 2D neural network object\n",
    "class NeuralNet2D(object):\n",
    "    \n",
    "    def __init__(self, sess, dim1, dim2, label_size):\n",
    "        self.sess = sess\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None, dim1*dim2])\n",
    "        self.layers = [tf.reshape(self.x, [-1,dim1,dim2,1])]\n",
    "        self.layer_index = 0\n",
    "        self.y_ = tf.placeholder(tf.float32, shape=[None, label_size])\n",
    "    \n",
    "    def add_convolution(self, dim1, dim2, stride1, stride2, output_channels, padding='SAME'):\n",
    "        input_channels = self.layers[self.layer_index].get_shape().as_list()[-1]\n",
    "\n",
    "        self.layers.append(tf.nn.relu(tf.nn.conv2d(self.layers[self.layer_index],\n",
    "                                                   weight_variable([dim1, dim2, input_channels, output_channels]),\n",
    "                                                   strides=[1, stride1, stride2, 1],\n",
    "                                                   padding=padding) + bias_variable([output_channels])))\n",
    "        self.layer_index += 1\n",
    "\n",
    "    def add_max_pool(self, dim1, dim2, stride1, stride2, padding='SAME'):\n",
    "        self.layers.append(tf.nn.max_pool(self.layers[self.layer_index],\n",
    "                                            ksize=[1, dim1, dim2, 1],\n",
    "                                            strides=[1, stride1, stride2, 1], padding=padding))\n",
    "        \n",
    "        self.layer_index += 1\n",
    "    \n",
    "    def add_fully_connected(self, num_nodes):\n",
    "        dim = self.layers[self.layer_index].get_shape().as_list()\n",
    "        dim = dim[1] * dim[2] * dim[3]\n",
    "        self.layers.append(tf.nn.relu(tf.matmul(tf.reshape(self.layers[self.layer_index], [-1, dim]),\n",
    "                                                weight_variable([dim, num_nodes])) + bias_variable([num_nodes])))\n",
    "        self.layer_index += 1\n",
    "    \n",
    "    def add_dropout(self, num_nodes):\n",
    "        dim = self.layers[self.layer_index].get_shape().as_list()\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        self.layers.append(tf.matmul(tf.nn.dropout(self.layers[self.layer_index], self.keep_prob),\n",
    "                                     weight_variable([dim[-1], num_nodes])) + bias_variable([num_nodes]))\n",
    "        self.layer_index += 1\n",
    "    \n",
    "    def make_train_step(self, problem_type):\n",
    "        current_layer = self.layers[self.layer_index]\n",
    "        if problem_type == 'classification':\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(current_layer,\n",
    "                                                                                   self.y_))\n",
    "            self.train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "            \n",
    "            correct_prediction = tf.equal(tf.argmax(current_layer,1), tf.argmax(self.y_,1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            \n",
    "        elif problem_type == 'regression':\n",
    "            SS_err = tf.reduce_sum(tf.square(tf.sub(current_layer, self.y_)))\n",
    "            SS_tot = tf.reduce_sum(tf.square(tf.sub(self.y_, tf.reduce_mean(self.y_))))\n",
    "            R_2 = tf.sub(tf.cast(1.0, tf.float32), tf.div(SS_err, SS_tot))\n",
    "\n",
    "            self.train_step = tf.train.AdamOptimizer(1e-4).minimize(SS_err)\n",
    "            self.accuracy = R_2\n",
    "        \n",
    "        else:\n",
    "            print('problem_type must be \\'classification\\' or \\'regression\\'')\n",
    "            \n",
    "    \n",
    "    def train_model(self, train, test, num_epoch=20000, batch_size=50, \n",
    "                    report_int=1000, keep_prob_train=0.5):\n",
    "\n",
    "        # initialize variables\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train epochs\n",
    "        for i in range(num_epoch):\n",
    "            batch = train.next_batch(batch_size)\n",
    "            if i%report_int == 0:\n",
    "                train_accuracy = self.accuracy.eval(feed_dict={self.x: batch[0],\n",
    "                                                               self.y_: batch[1],\n",
    "                                                               self.keep_prob: 1.0})\n",
    "\n",
    "                print(\"step %d, training accuracy %g\"%(i, train_accuracy),\n",
    "                      \"test accuracy %g\"%self.accuracy.eval(feed_dict={self.x: test.features,\n",
    "                                                                       self.y_: test.labels,\n",
    "                                                                       self.keep_prob: 1.0}))\n",
    "            self.train_step.run(feed_dict={self.x: batch[0],\n",
    "                                           self.y_: batch[1],\n",
    "                                           self.keep_prob: keep_prob_train})\n",
    "\n",
    "        print(\"test accuracy %g\"%self.accuracy.eval(feed_dict={self.x: test.features,\n",
    "                                                               self.y_: test.labels,\n",
    "                                                               self.keep_prob: 1.0}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to distinguish complementary sequences from random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate complementary and random 4mers\n",
    "features = np.zeros((1000, 256))\n",
    "labels = np.zeros((1000, 2))\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_match_pair(4)\n",
    "        labels[i,:] = [1, 0]\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(4,4)\n",
    "        labels[i,:] = [0, 1]\n",
    "\n",
    "    features[i,:] = make_square(seq1, seq2).flatten()\n",
    "\n",
    "match_train = Dataset(features[:900, :], labels[:900, :])\n",
    "match_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.58 test accuracy 0.51\n",
      "step 1000, training accuracy 1 test accuracy 0.98\n",
      "step 2000, training accuracy 1 test accuracy 0.99\n",
      "step 3000, training accuracy 1 test accuracy 0.99\n",
      "step 4000, training accuracy 1 test accuracy 0.99\n",
      "test accuracy 0.99\n"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 16, 16\n",
    "num_output1 = 4\n",
    "fully_connected_nodes = 1024\n",
    "out_nodes = 2\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('classification')\n",
    "NN.train_model(match_train, match_test, num_epoch=5000,\n",
    "               batch_size=50, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to distinguish seed-matched sequences from random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate mirna,utr pairs of length 20 with a seed match\n",
    "features = np.zeros((1000, 6400))\n",
    "labels = np.zeros((1000,2))\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_seed_match_pair(20,20)\n",
    "        labels[i,:] = [1, 0]\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(20,20)\n",
    "        labels[i,:] = [0, 1]\n",
    "    \n",
    "    features[i,:] = make_square(seq1, seq2).flatten()\n",
    "\n",
    "seed_match_train = Dataset(features[:900, :], labels[:900, :])\n",
    "seed_match_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.5 test accuracy 0.52\n",
      "step 1000, training accuracy 1 test accuracy 0.95\n",
      "step 2000, training accuracy 1 test accuracy 0.95\n",
      "step 3000, training accuracy 1 test accuracy 0.95\n",
      "step 4000, training accuracy 1 test accuracy 0.96\n",
      "test accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 80, 80\n",
    "num_output1 = 4\n",
    "fully_connected_nodes = 1024\n",
    "out_nodes = 2\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('classification')\n",
    "NN.train_model(seed_match_train, seed_match_test, num_epoch=5000,\n",
    "               batch_size=50, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to regress on seed pairing stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_sps(seq):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ==========\n",
    "    seq: string, consist of A, U, C, G\n",
    "    \n",
    "    Returns:\n",
    "    =======\n",
    "    float: seed pairing stability\n",
    "    \"\"\"\n",
    "    thermo_dict = {'AA':-0.93,'AU':-1.10,'AC':-2.24,'AG':-2.08,\n",
    "               'UA':-1.33,'UU':-0.93,'UC':-2.35,'UG':-2.11,\n",
    "               'CA':-2.11,'CU':-2.08,'CC':-3.26,'CG':-2.36,\n",
    "               'GA':-2.35,'GU':-2.24,'GC':-3.42,'GG':-3.26}\n",
    "    init = 4.09\n",
    "    terminal_au = 0.45\n",
    "    \n",
    "    # initialize score\n",
    "    score = init\n",
    "    \n",
    "    # add score for each dinucleotide\n",
    "    for i in range(len(seq)-1):\n",
    "        score += thermo_dict[seq[i:i+2]]\n",
    "    \n",
    "    # add score for each terminal AU\n",
    "    score += terminal_au*((seq[0]+seq[-1]).count('A') + (seq[0]+seq[-1]).count('U'))\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random sequences\n",
    "features = np.zeros((1000, 6400))\n",
    "labels = np.zeros((1000,1))\n",
    "seq1s, seq2s = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_seed_match_pair(20,20)\n",
    "        seq1s.append(seq1)\n",
    "        seq2s.append(seq2)\n",
    "        labels[i,:] = calculate_sps(seq1[1:7]) + (np.random.random()-0.5)\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(20,20)\n",
    "        seq1s.append(seq1)\n",
    "        seq2s.append(seq2)\n",
    "        labels[i,:] = np.random.random()-0.5\n",
    "    \n",
    "    features[i,:] = make_square(seq1, seq2).flatten()\n",
    "\n",
    "seed_match_sps_train = Dataset(features[:900, :], labels[:900, :])\n",
    "seed_match_sps_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy -1.04116 test accuracy -0.901628\n",
      "step 1000, training accuracy 0.540802 test accuracy 0.367427\n",
      "step 2000, training accuracy 0.942358 test accuracy 0.861274\n",
      "step 3000, training accuracy 0.981681 test accuracy 0.913068\n",
      "step 4000, training accuracy 0.988818 test accuracy 0.926939\n",
      "step 5000, training accuracy 0.987021 test accuracy 0.925711\n",
      "step 6000, training accuracy 0.985119 test accuracy 0.928617\n",
      "step 7000, training accuracy 0.99206 test accuracy 0.936477\n",
      "step 8000, training accuracy 0.996499 test accuracy 0.941244\n",
      "step 9000, training accuracy 0.994621 test accuracy 0.942679\n",
      "test accuracy 0.944907\n"
     ]
    }
   ],
   "source": [
    "dim1, dim2 = 80, 80\n",
    "num_output1 = 16\n",
    "num_output2 = 16\n",
    "fully_connected_nodes = 32\n",
    "out_nodes = 1\n",
    "\n",
    "NN = NeuralNet2D(sess, dim1, dim2, out_nodes)\n",
    "NN.add_convolution(4, 4, 4, 4, num_output1)\n",
    "NN.add_convolution(2, 2, 1, 1, num_output2)\n",
    "NN.add_fully_connected(fully_connected_nodes)\n",
    "NN.add_dropout(out_nodes)\n",
    "NN.make_train_step('regression')\n",
    "NN.train_model(seed_match_sps_train, seed_match_sps_test, num_epoch=10000,\n",
    "               batch_size=100, report_int=1000, keep_prob_train=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regression on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCCGGUAAU'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LOGFC_FILE1 = '../data/Supplementary1.csv'\n",
    "LOGFC_FILE2 = '../data/Supplementary2.csv'\n",
    "GENE_FILE = '../data/Gene_info.txt'\n",
    "SEED_FILE = '../data/seed_dict.csv'\n",
    "SEQ_FILE = '../data/seq_dict.csv'\n",
    "\n",
    "def rev_comp(seq):\n",
    "    \"\"\"Get the reverse complement of a given RNA sequence\"\"\"\n",
    "    \n",
    "    intab = \"AUCG\"\n",
    "    outtab = \"UAGC\"\n",
    "    trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "    return seq.translate(trantab)[::-1]\n",
    "    \n",
    "rev_comp('AUUACCGGC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>mirna_seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCGUAGG</th>\n",
       "      <td>1595297366</td>\n",
       "      <td>UUCGUAGGUCAAAAUACACAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCAUCUC</th>\n",
       "      <td>1595297383</td>\n",
       "      <td>UUCAUCUCCAAUUCGUAGGAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UGCUCUU</th>\n",
       "      <td>1595297389</td>\n",
       "      <td>AUGCUCUUUCCUCCUGUGCAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UUUGGAA</th>\n",
       "      <td>1595297394</td>\n",
       "      <td>UUUUGGAACAGUCUUUCCGAAAAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UUGGAAC</th>\n",
       "      <td>1595297399</td>\n",
       "      <td>UUUGGAACAGUCUUUCCGAAAAAA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                col                 mirna_seq\n",
       "seed                                         \n",
       "UCGUAGG  1595297366  UUCGUAGGUCAAAAUACACAAAAA\n",
       "UCAUCUC  1595297383  UUCAUCUCCAAUUCGUAGGAAAAA\n",
       "UGCUCUU  1595297389  AUGCUCUUUCCUCCUGUGCAAAAA\n",
       "UUUGGAA  1595297394  UUUUGGAACAGUCUUUCCGAAAAA\n",
       "UUGGAAC  1595297399  UUUGGAACAGUCUUUCCGAAAAAA"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENE_INFO = pd.read_csv(GENE_FILE,sep='\\t').drop(['Gene description','Species ID'],1)\n",
    "GENE_INFO = GENE_INFO.groupby('Gene symbol').agg(lambda x:tuple(x))\n",
    "GENE_INFO.loc[:,'Isoform ratio'] = [float(max(x))/np.nansum(x) for x in GENE_INFO['3P-seq tags + 5']]\n",
    "GENE_INFO.loc[:,'Transcript ID'] = [x[y.index(1)] for (x,y) in zip(GENE_INFO['Transcript ID'],GENE_INFO['Representative transcript?'])]\n",
    "GENE_INFO = GENE_INFO[['Transcript ID','Isoform ratio']]\n",
    "GENE_INFO_TRANSCRIPT = GENE_INFO.reset_index().set_index('Transcript ID')\n",
    "\n",
    "SEED_INFO = pd.read_csv(SEED_FILE,sep='\\t')\n",
    "SEED_DICT = {}\n",
    "for row in SEED_INFO.iterrows():\n",
    "    SEED_DICT[row[1]['col']] = row[1]['seed']\n",
    "\n",
    "SEEDS = sorted(list(SEED_DICT.values()))\n",
    "\n",
    "SEQ_INFO = pd.read_csv(SEQ_FILE, sep='\\t')\n",
    "SEQ_INFO['seed'] = [SEED_DICT[x] for x in SEQ_INFO['col']]\n",
    "SEQ_INFO = SEQ_INFO.set_index('seed')\n",
    "SEQ_INFO['mirna_seq'] = [(x + 'AAAAAAA')[:24] for x in SEQ_INFO['mirna_seq']]\n",
    "SEQ_INFO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTR sequence</th>\n",
       "      <th>UTR length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensembl ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CDR1as</th>\n",
       "      <td>GUUUCCGAUGGCACCUGUGUCAAGGUCUUCCAACAACUCCGGGUCU...</td>\n",
       "      <td>1485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000233.5</th>\n",
       "      <td>CCAGCCAGGGGCAGGCCCCUGAUGCCCGGAAGCUCCUGCGUGCAUC...</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000412.3</th>\n",
       "      <td>AUUGCACUUUAUAUGUCCAGCCUCUUCCUCAGUCCCCCAAACCAAA...</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001008.4</th>\n",
       "      <td>CCCCUCUCCACCAGCCCUACUCCUGCGGCUGCCUGCCCCCCAGUCU...</td>\n",
       "      <td>2163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001146.2</th>\n",
       "      <td>CCCAAGACCCACCCGCCUCAGCCCAGCCCAGGCAGCGGGGUGGUGG...</td>\n",
       "      <td>3001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        UTR sequence  \\\n",
       "Ensembl ID                                                             \n",
       "CDR1as             GUUUCCGAUGGCACCUGUGUCAAGGUCUUCCAACAACUCCGGGUCU...   \n",
       "ENST00000000233.5  CCAGCCAGGGGCAGGCCCCUGAUGCCCGGAAGCUCCUGCGUGCAUC...   \n",
       "ENST00000000412.3  AUUGCACUUUAUAUGUCCAGCCUCUUCCUCAGUCCCCCAAACCAAA...   \n",
       "ENST00000001008.4  CCCCUCUCCACCAGCCCUACUCCUGCGGCUGCCUGCCCCCCAGUCU...   \n",
       "ENST00000001146.2  CCCAAGACCCACCCGCCUCAGCCCAGCCCAGGCAGCGGGGUGGUGG...   \n",
       "\n",
       "                   UTR length  \n",
       "Ensembl ID                     \n",
       "CDR1as                   1485  \n",
       "ENST00000000233.5         422  \n",
       "ENST00000000412.3        1457  \n",
       "ENST00000001008.4        2163  \n",
       "ENST00000001146.2        3001  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTRS = pd.read_csv('../../05_TargetPrediction/targetscan_files/UTR_Sequences_Ensembl_Human.txt',\n",
    "                   sep='\\t', usecols=['Ensembl ID','UTR sequence']).set_index('Ensembl ID')\n",
    "UTRS['UTR sequence'] = [x.replace('-','').upper().replace('T','U') for x in UTRS['UTR sequence']]\n",
    "UTRS['UTR length'] = [len(x) for x in UTRS['UTR sequence']]\n",
    "UTRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6426\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UCGUAGG</th>\n",
       "      <th>UCAUCUC</th>\n",
       "      <th>UGCUCUU</th>\n",
       "      <th>UUUGGAA</th>\n",
       "      <th>UUGGAAC</th>\n",
       "      <th>CAAACAC</th>\n",
       "      <th>AAUACAC</th>\n",
       "      <th>UUUCCUC</th>\n",
       "      <th>AGCUUCC</th>\n",
       "      <th>AGUCAGA</th>\n",
       "      <th>...</th>\n",
       "      <th>AAGGCAC</th>\n",
       "      <th>AGCAGCA</th>\n",
       "      <th>AAAGUGC</th>\n",
       "      <th>AACACUG</th>\n",
       "      <th>AAUACUG</th>\n",
       "      <th>UGACCUA</th>\n",
       "      <th>GAGGUAG</th>\n",
       "      <th>GCAGCAU</th>\n",
       "      <th>Gene symbol</th>\n",
       "      <th>Isoform ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000318602.7</th>\n",
       "      <td>0.128</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A2M</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000401850.1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.175</td>\n",
       "      <td>-0.286</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.159</td>\n",
       "      <td>A4GALT</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000236709.3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.461</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>A4GNT</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000209873.4</th>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.081</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.153</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.033</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.093</td>\n",
       "      <td>AAAS</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000337664.4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.136</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.118</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.023</td>\n",
       "      <td>AADAT</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UCGUAGG  UCAUCUC  UGCUCUU  UUUGGAA  UUGGAAC  CAAACAC  \\\n",
       "ENST00000318602.7    0.128   -0.075   -0.113    0.066   -0.053     0.15   \n",
       "ENST00000401850.1      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000236709.3      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000209873.4    0.017   -0.081   -0.043   -0.023    0.153    -0.06   \n",
       "ENST00000337664.4      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "                   AAUACAC  UUUCCUC  AGCUUCC  AGUCAGA      ...        AAGGCAC  \\\n",
       "ENST00000318602.7   -0.007   -0.024   -0.035    0.051      ...          0.239   \n",
       "ENST00000401850.1      NaN      NaN      NaN      NaN      ...         -0.175   \n",
       "ENST00000236709.3      NaN      NaN      NaN      NaN      ...            NaN   \n",
       "ENST00000209873.4   -0.026    0.061   -0.042    0.028      ...         -0.063   \n",
       "ENST00000337664.4      NaN      NaN      NaN      NaN      ...            NaN   \n",
       "\n",
       "                   AGCAGCA  AAAGUGC  AACACUG  AAUACUG  UGACCUA  GAGGUAG  \\\n",
       "ENST00000318602.7      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000401850.1   -0.286    0.052    0.195   -0.133    0.093    0.036   \n",
       "ENST00000236709.3   -0.461    0.080    0.011   -0.037    0.167    0.050   \n",
       "ENST00000209873.4   -0.144   -0.014    0.033   -0.059    0.046   -0.012   \n",
       "ENST00000337664.4   -0.136   -0.143    0.003    0.118   -0.317    0.380   \n",
       "\n",
       "                   GCAGCAU  Gene symbol  Isoform ratio  \n",
       "ENST00000318602.7      NaN          A2M            1.0  \n",
       "ENST00000401850.1    0.159       A4GALT            1.0  \n",
       "ENST00000236709.3   -0.130        A4GNT            1.0  \n",
       "ENST00000209873.4    0.093         AAAS            1.0  \n",
       "ENST00000337664.4    0.023        AADAT            1.0  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFCs1 = pd.read_csv(LOGFC_FILE1)\n",
    "logFCs1 = logFCs1[logFCs1['Used in training'] == 'yes']\n",
    "logFCs1['Transcript ID'] = list(GENE_INFO.loc[logFCs1['Gene symbol']]['Transcript ID'])\n",
    "logFCs1 = logFCs1.drop(['Used in training','RefSeq ID','Gene symbol'],1).dropna(subset=['Transcript ID'])\n",
    "logFCs1 = logFCs1.drop_duplicates(subset=['Transcript ID']).set_index('Transcript ID')\n",
    "logFCs1.columns = [SEED_DICT[x] if x in SEED_DICT else x for x in logFCs1.columns]\n",
    "\n",
    "logFCs2 = pd.read_csv(LOGFC_FILE2)\n",
    "logFCs2['Transcript ID'] = list(GENE_INFO.loc[logFCs2['Gene symbol']]['Transcript ID'])\n",
    "logFCs2 = logFCs2.drop(['RefSeq ID','Gene symbol'],1).dropna(subset=['Transcript ID'])\n",
    "logFCs2 = logFCs2.drop_duplicates(subset=['Transcript ID']).set_index('Transcript ID')\n",
    "logFCs2.columns = [SEED_DICT[x] if x in SEED_DICT else x for x in logFCs2.columns]\n",
    "\n",
    "logFCs = pd.concat([logFCs1,logFCs2],axis=1,join='outer')\n",
    "logFCs = pd.concat([logFCs, GENE_INFO_TRANSCRIPT], axis=1, join='inner')\n",
    "\n",
    "logFCs = logFCs[logFCs['Isoform ratio'] > 0.9]\n",
    "print(len(logFCs))\n",
    "logFCs.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
