{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow on RNA sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# open a tensorflow session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create dataset object that holds features and labels and cycles through the data in batches\n",
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, features, labels):\n",
    "        assert (len(features) == len(labels))\n",
    "        self.features = np.array(features)\n",
    "        self.labels = np.array(labels)\n",
    "        self.index = 0\n",
    "        self.size = len(labels)\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        old_index = self.index\n",
    "        new_index = self.index + batch_size\n",
    "        self.index = new_index % self.size\n",
    "        if new_index <= self.size:\n",
    "            return (self.features[old_index: new_index], self.labels[old_index: new_index])\n",
    "        else:\n",
    "            subfeatures = np.concatenate([self.features[old_index:], self.features[:self.index]])\n",
    "            sublabels = np.concatenate([self.labels[old_index:], self.labels[:self.index]])\n",
    "            return (subfeatures, sublabels)\n",
    "    \n",
    "    def reset_index(self):\n",
    "        self.index = 0    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # create an object that implements a basic neural network for classification\n",
    "# class Classify(object):\n",
    "    \n",
    "#     def __init__(self, sess, feat_length, label_length):\n",
    "#         self.sess = sess\n",
    "#         self.feat_length = feat_length\n",
    "#         self.label_length = label_length\n",
    "\n",
    "#         # create variables for the features, labels, and weights\n",
    "#         self.x = tf.placeholder(tf.float32, shape=[None, feat_length])\n",
    "#         self.y_ = tf.placeholder(tf.float32, shape=[None, label_length])\n",
    "#         self.W = tf.Variable(tf.zeros([feat_length,label_length]))\n",
    "#         self.b = tf.Variable(tf.zeros([label_length]))\n",
    "        \n",
    "#     def make_model(self, func, loss_func, optim_func):\n",
    "#         self.y = func(self.x, self.W, self.b)\n",
    "#         self.cross_entropy = loss_func(self.y_, self.y)\n",
    "#         self.train_step = optim_func(self.cross_entropy)\n",
    "        \n",
    "#         self.correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(self.y_,1))\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "    \n",
    "#     def train(self, num_epoch, batch_size, train_data):\n",
    "#         self.sess.run(tf.initialize_all_variables())\n",
    "#         for i in range(num_epoch):\n",
    "#             batch = train_data.next_batch(batch_size)\n",
    "#             self.train_step.run(feed_dict={self.x: batch[0], self.y_: batch[1]})\n",
    "        \n",
    "#     def test(self, test_data):\n",
    "#         print(self.accuracy.eval(feed_dict={self.x: test_data.features, self.y_: test_data.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import MNIST data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# import sample data for digit recognition\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_train = Dataset(mnist.train.images, mnist.train.labels)\n",
    "mnist_test = Dataset(mnist.test.images, mnist.test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def train_model(sess, train_step, eval_var, num_epoch, batch_size, report_int, train, test):\n",
    "\n",
    "    # initialize variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train epochs\n",
    "    for i in range(num_epoch):\n",
    "        batch = train.next_batch(50)\n",
    "        if i%report_int == 0:\n",
    "            train_accuracy = eval_var.eval(feed_dict={x:batch[0],\n",
    "                                                      y_: batch[1],\n",
    "                                                      keep_prob: 1.0})\n",
    "\n",
    "            print(\"step %d, training accuracy %g\"%(i, train_accuracy),\n",
    "                  \"test accuracy %g\"%eval_var.eval(feed_dict={x: test.features,\n",
    "                                                              y_: test.labels,\n",
    "                                                              keep_prob: 1.0}))\n",
    "        train_step.run(feed_dict={x: batch[0],\n",
    "                                  y_: batch[1],\n",
    "                                  keep_prob: 0.5})\n",
    "\n",
    "    print(\"test accuracy %g\"%eval_var.eval(feed_dict={x: test.features,\n",
    "                                                      y_: test.labels,\n",
    "                                                      keep_prob: 1.0}))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Convolutional_2D_NN(object):\n",
    "    \n",
    "#     def __init__(self, sess, input_size, output_size):\n",
    "#         self.x = tf.placeholder(tf.float32, shape=[None, input_size*input_size])\n",
    "#         self.x_image = tf.reshape(x, [-1,input_size,input_size,1])\n",
    "#         self.y_ = tf.placeholder(tf.float32, shape=[None, output_size])\n",
    "\n",
    "#         self.current_layer = self.x_image\n",
    "    \n",
    "#     def add_layer(self, layer_func):\n",
    "#         self.current_layer = layer_func(self.current_layer)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First implement MNIST with a convolutional NN using tensorflow tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_size = 28\n",
    "final_size = 7\n",
    "num_output1 = 32\n",
    "num_output2 = 64\n",
    "fully_connected_nodes = 1024\n",
    "out_nodes = 10\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, init_size*init_size])\n",
    "x_image = tf.reshape(x, [-1,init_size,init_size,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([final_size * final_size * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, final_size * final_size * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08 test accuracy 0.0731\n",
      "step 100, training accuracy 0.66 test accuracy 0.7809\n",
      "step 200, training accuracy 0.9 test accuracy 0.8655\n",
      "step 300, training accuracy 0.92 test accuracy 0.9016\n",
      "step 400, training accuracy 0.9 test accuracy 0.915\n",
      "step 500, training accuracy 0.94 test accuracy 0.9169\n",
      "step 600, training accuracy 0.94 test accuracy 0.9289\n",
      "step 700, training accuracy 1 test accuracy 0.9345\n",
      "step 800, training accuracy 0.92 test accuracy 0.9444\n",
      "step 900, training accuracy 0.98 test accuracy 0.9393\n",
      "test accuracy 0.9464\n"
     ]
    }
   ],
   "source": [
    "# use a cross entropy loss function and optimize with Adam\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_model(sess, train_step, accuracy, 1000, 50, 100, mnist_train, mnist_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classify RNA sequences based on complementarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_random_sequence(length):\n",
    "    \"\"\"Generate a random RNA sequence of a given length\"\"\"\n",
    "    \n",
    "    nts = ['A','U','C','G']\n",
    "    sequence = np.random.choice(nts, size=length, replace=True)\n",
    "\n",
    "    return ''.join(sequence)\n",
    "\n",
    "def get_complementary(seq):\n",
    "    \"\"\"Get the complementary sequence of a given RNA sequence\"\"\"\n",
    "    \n",
    "    intab = \"AUCG\"\n",
    "    outtab = \"UAGC\"\n",
    "    trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "    return seq.translate(trantab)\n",
    "\n",
    "def generate_match_pair(length, random_seed=None):\n",
    "    \"\"\"Generate two sequences that are base-paired\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    seq1 = generate_random_sequence(length)\n",
    "    seq2 = get_complementary(seq1)\n",
    "    \n",
    "    return seq1, seq2\n",
    "\n",
    "def generate_seed_match_pair(length1, length2, random_seed=None):\n",
    "    \"\"\"Generate two sequences that are base-paired at positions 1-7\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    seq1 = generate_random_sequence(length1)\n",
    "    up_fragment = generate_random_sequence(1)\n",
    "    down_fragment = generate_random_sequence(length2-7)\n",
    "    mid_fragment = get_complementary(seq1[1:7])\n",
    "    \n",
    "    seq2 = up_fragment + mid_fragment + down_fragment\n",
    "    \n",
    "    return seq1, seq2\n",
    "\n",
    "def generate_random_pair(length1, length2, random_seed=None):\n",
    "    \"\"\"Generate two random sequences that are not perfectly complementary\"\"\"\n",
    "    \n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    seq1 = generate_random_sequence(length1)\n",
    "    match_seq1 = get_complementary(seq1)\n",
    "\n",
    "    while True:\n",
    "        seq2 = generate_random_sequence(length2)\n",
    "\n",
    "        if match_seq1 != seq2:\n",
    "            return seq1, seq2\n",
    "\n",
    "def one_hot_encode(seq, nt_order):\n",
    "    \"\"\"Convert RNA sequence to one-hot encoding\"\"\"\n",
    "    \n",
    "    one_hot = [list(np.array(nt_order == nt, dtype=int)) for nt in seq]\n",
    "    one_hot = [item for sublist in one_hot for item in sublist]\n",
    "    \n",
    "    return np.array(one_hot)\n",
    "\n",
    "def make_square(seq1, seq2):\n",
    "    \"\"\"Given two sequences, calculate outer product of one-hot encodings\"\"\"\n",
    "\n",
    "    return np.outer(one_hot_encode(seq1, np.array(['A','U','C','G'])),\n",
    "                    one_hot_encode(seq2, np.array(['U','A','G','C']))).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GCGUGCGCCCGAAGUAUGCA', 'GGCACGCAACGUGGCUCGCC')\n",
      "('CACAAAGGUUGACUUUGCGU', 'UUAGAAGAGAGUAAUGACCC')\n",
      "('GCGGACUAGCGCACAUCCGG', 'CGCCUGAUCGCGUGUAGGCC')\n",
      "[[0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(generate_seed_match_pair(20,20))\n",
    "print(generate_random_pair(20,20))\n",
    "print(generate_match_pair(20,20))\n",
    "print(make_square('AAA','AAA').reshape(12,12))\n",
    "print(make_square('UAG','AUC').reshape(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add another convolution function that looks at 4x4 blocks without overlapping\n",
    "def conv2d_4step(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 4, 4, 1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to distinguish complementary sequences from random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate complementary and random 4mers\n",
    "features = np.zeros((1000, 256))\n",
    "labels = np.zeros((1000, 2))\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_match_pair(4)\n",
    "        labels[i,:] = [1, 0]\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(4,4)\n",
    "        labels[i,:] = [0, 1]\n",
    "\n",
    "    features[i,:] = make_square(seq1, seq2)\n",
    "\n",
    "match_train = Dataset(features[:900, :], labels[:900, :])\n",
    "match_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_size = 16\n",
    "final_size = 1\n",
    "num_output1 = 4\n",
    "num_output2 = 8\n",
    "fully_connected_nodes = 512\n",
    "out_nodes = 2\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, init_size*init_size])\n",
    "x_image = tf.reshape(x, [-1,init_size,init_size,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d_4step(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([final_size * final_size * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, final_size * final_size * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.44 test accuracy 0.43\n",
      "step 1000, training accuracy 0.82 test accuracy 0.84\n",
      "step 2000, training accuracy 0.96 test accuracy 0.92\n",
      "step 3000, training accuracy 0.96 test accuracy 0.98\n",
      "step 4000, training accuracy 0.98 test accuracy 0.99\n",
      "step 5000, training accuracy 0.94 test accuracy 1\n",
      "step 6000, training accuracy 1 test accuracy 1\n",
      "step 7000, training accuracy 0.94 test accuracy 1\n",
      "step 8000, training accuracy 0.98 test accuracy 1\n",
      "step 9000, training accuracy 1 test accuracy 1\n",
      "step 10000, training accuracy 1 test accuracy 1\n",
      "step 11000, training accuracy 1 test accuracy 1\n",
      "step 12000, training accuracy 1 test accuracy 1\n",
      "step 13000, training accuracy 1 test accuracy 1\n",
      "step 14000, training accuracy 0.98 test accuracy 1\n",
      "step 15000, training accuracy 1 test accuracy 1\n",
      "step 16000, training accuracy 0.94 test accuracy 1\n",
      "step 17000, training accuracy 0.98 test accuracy 1\n",
      "step 18000, training accuracy 1 test accuracy 1\n",
      "step 19000, training accuracy 1 test accuracy 1\n",
      "test accuracy 1\n"
     ]
    }
   ],
   "source": [
    "# use a cross entropy loss function and optimize with Adam\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_model(sess, train_step, accuracy, 20000, 50, 1000, match_train, match_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to distinguish seed-matched sequences from random sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate mirna,utr pairs of length 20 with a seed match\n",
    "features = np.zeros((1000, 6400))\n",
    "labels = np.zeros((1000,2))\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_seed_match_pair(20,20)\n",
    "        labels[i,:] = [1, 0]\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(20,20)\n",
    "        labels[i,:] = [0, 1]\n",
    "    \n",
    "    features[i,:] = make_square(seq1, seq2)\n",
    "\n",
    "seed_match_train = Dataset(features[:900, :], labels[:900, :])\n",
    "seed_match_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_size = 80\n",
    "final_size = 5\n",
    "num_output1 = 8\n",
    "num_output2 = 16\n",
    "fully_connected_nodes = 512\n",
    "out_nodes = 2\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, init_size*init_size])\n",
    "x_image = tf.reshape(x, [-1,init_size,init_size,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d_4step(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([final_size * final_size * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, final_size * final_size * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use a cross entropy loss function and optimize with Adam\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "train_model(sess, train_step, accuracy, 10000, 50, 1000, seed_match_train, seed_match_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn to regress on seed pairing stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_sps(seq):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    ==========\n",
    "    seq: string, consist of A, U, C, G\n",
    "    \n",
    "    Returns:\n",
    "    =======\n",
    "    float: seed pairing stability\n",
    "    \"\"\"\n",
    "    thermo_dict = {'AA':-0.93,'AU':-1.10,'AC':-2.24,'AG':-2.08,\n",
    "               'UA':-1.33,'UU':-0.93,'UC':-2.35,'UG':-2.11,\n",
    "               'CA':-2.11,'CU':-2.08,'CC':-3.26,'CG':-2.36,\n",
    "               'GA':-2.35,'GU':-2.24,'GC':-3.42,'GG':-3.26}\n",
    "    init = 4.09\n",
    "    terminal_au = 0.45\n",
    "    \n",
    "    # initialize score\n",
    "    score = init\n",
    "    \n",
    "    # add score for each dinucleotide\n",
    "    for i in range(len(seq)-1):\n",
    "        score += thermo_dict[seq[i:i+2]]\n",
    "    \n",
    "    # add score for each terminal AU\n",
    "    score += terminal_au*((seq[0]+seq[-1]).count('A') + (seq[0]+seq[-1]).count('U'))\n",
    "    \n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random sequences\n",
    "features = np.zeros((1000, 6400))\n",
    "labels = np.zeros((1000,1))\n",
    "seq1s, seq2s = [], []\n",
    "\n",
    "for i in range(1000):\n",
    "    if np.random.random() < 0.5:\n",
    "        seq1, seq2 = generate_seed_match_pair(20,20)\n",
    "        seq1s.append(seq1)\n",
    "        seq2s.append(seq2)\n",
    "        labels[i,:] = calculate_sps(seq1[1:7]) + (np.random.random()-0.5)\n",
    "    else:\n",
    "        seq1, seq2 = generate_random_pair(20,20)\n",
    "        seq1s.append(seq1)\n",
    "        seq2s.append(seq2)\n",
    "        labels[i,:] = np.random.random()-0.5\n",
    "    \n",
    "    features[i,:] = make_square(seq1, seq2)\n",
    "\n",
    "seed_match_sps_train = Dataset(features[:900, :], labels[:900, :])\n",
    "seed_match_sps_test = Dataset(features[900:, :], labels[900:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_size = 80\n",
    "final_size = 5\n",
    "num_output1 = 16\n",
    "num_output2 = 32\n",
    "fully_connected_nodes = 512\n",
    "out_nodes = 1\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, init_size*init_size])\n",
    "x_image = tf.reshape(x, [-1,init_size,init_size,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d_4step(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([final_size * final_size * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, final_size * final_size * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use an L2 loss and Adam optimizer\n",
    "SS_err = tf.reduce_sum(tf.square(tf.sub(y_conv, y_)))\n",
    "SS_tot = tf.reduce_sum(tf.square(tf.sub(y_, tf.reduce_mean(y_))))\n",
    "R_2 = tf.sub(tf.cast(1.0, tf.float32), tf.div(SS_err, SS_tot))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(SS_err)\n",
    "\n",
    "train_model(sess, train_step, R_2, 20000, 50, 1000, seed_match_sps_train, seed_match_sps_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Regression on real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCCGGUAAU'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "LOGFC_FILE1 = '../data/Supplementary1.csv'\n",
    "LOGFC_FILE2 = '../data/Supplementary2.csv'\n",
    "GENE_FILE = '../data/Gene_info.txt'\n",
    "SEED_FILE = '../data/seed_dict.csv'\n",
    "SEQ_FILE = '../data/seq_dict.csv'\n",
    "\n",
    "def rev_comp(seq):\n",
    "    \"\"\"Get the reverse complement of a given RNA sequence\"\"\"\n",
    "    \n",
    "    intab = \"AUCG\"\n",
    "    outtab = \"UAGC\"\n",
    "    trantab = str.maketrans(intab, outtab)\n",
    "\n",
    "    return seq.translate(trantab)[::-1]\n",
    "    \n",
    "rev_comp('AUUACCGGC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col</th>\n",
       "      <th>mirna_seq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>UCGUAGG</th>\n",
       "      <td>1595297366</td>\n",
       "      <td>UUCGUAGGUCAAAAUACAC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UCAUCUC</th>\n",
       "      <td>1595297383</td>\n",
       "      <td>UUCAUCUCCAAUUCGUAGG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UGCUCUU</th>\n",
       "      <td>1595297389</td>\n",
       "      <td>AUGCUCUUUCCUCCUGUGC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UUUGGAA</th>\n",
       "      <td>1595297394</td>\n",
       "      <td>UUUUGGAACAGUCUUUCCG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UUGGAAC</th>\n",
       "      <td>1595297399</td>\n",
       "      <td>UUUGGAACAGUCUUUCCGA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                col            mirna_seq\n",
       "seed                                    \n",
       "UCGUAGG  1595297366  UUCGUAGGUCAAAAUACAC\n",
       "UCAUCUC  1595297383  UUCAUCUCCAAUUCGUAGG\n",
       "UGCUCUU  1595297389  AUGCUCUUUCCUCCUGUGC\n",
       "UUUGGAA  1595297394  UUUUGGAACAGUCUUUCCG\n",
       "UUGGAAC  1595297399  UUUGGAACAGUCUUUCCGA"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GENE_INFO = pd.read_csv(GENE_FILE,sep='\\t').drop(['Gene description','Species ID'],1)\n",
    "GENE_INFO = GENE_INFO.groupby('Gene symbol').agg(lambda x:tuple(x))\n",
    "GENE_INFO.loc[:,'Isoform ratio'] = [float(max(x))/np.nansum(x) for x in GENE_INFO['3P-seq tags + 5']]\n",
    "GENE_INFO.loc[:,'Transcript ID'] = [x[y.index(1)] for (x,y) in zip(GENE_INFO['Transcript ID'],GENE_INFO['Representative transcript?'])]\n",
    "GENE_INFO = GENE_INFO[['Transcript ID','Isoform ratio']]\n",
    "\n",
    "SEED_INFO = pd.read_csv(SEED_FILE,sep='\\t')\n",
    "SEED_DICT = {}\n",
    "for row in SEED_INFO.iterrows():\n",
    "    SEED_DICT[row[1]['col']] = row[1]['seed']\n",
    "\n",
    "SEEDS = SEED_DICT.values()\n",
    "\n",
    "SEQ_INFO = pd.read_csv(SEQ_FILE, sep='\\t')\n",
    "SEQ_INFO['seed'] = [SEED_DICT[x] for x in SEQ_INFO['col']]\n",
    "SEQ_INFO = SEQ_INFO.set_index('seed')\n",
    "SEQ_INFO.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UTR sequence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensembl ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CDR1as</th>\n",
       "      <td>GUUUCCGAUGGCACCUGUGUCAAGGUCUUCCAACAACUCCGGGUCU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000233.5</th>\n",
       "      <td>CCAGCCAGGGGCAGGCCCCUGAUGCCCGGAAGCUCCUGCGUGCAUC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000000412.3</th>\n",
       "      <td>AUUGCACUUUAUAUGUCCAGCCUCUUCCUCAGUCCCCCAAACCAAA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001008.4</th>\n",
       "      <td>CCCCUCUCCACCAGCCCUACUCCUGCGGCUGCCUGCCCCCCAGUCU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001146.2</th>\n",
       "      <td>CCCAAGACCCACCCGCCUCAGCCCAGCCCAGGCAGCGGGGUGGUGG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        UTR sequence\n",
       "Ensembl ID                                                          \n",
       "CDR1as             GUUUCCGAUGGCACCUGUGUCAAGGUCUUCCAACAACUCCGGGUCU...\n",
       "ENST00000000233.5  CCAGCCAGGGGCAGGCCCCUGAUGCCCGGAAGCUCCUGCGUGCAUC...\n",
       "ENST00000000412.3  AUUGCACUUUAUAUGUCCAGCCUCUUCCUCAGUCCCCCAAACCAAA...\n",
       "ENST00000001008.4  CCCCUCUCCACCAGCCCUACUCCUGCGGCUGCCUGCCCCCCAGUCU...\n",
       "ENST00000001146.2  CCCAAGACCCACCCGCCUCAGCCCAGCCCAGGCAGCGGGGUGGUGG..."
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UTRS = pd.read_csv('../../05_TargetPrediction/targetscan_files/UTR_Sequences_Ensembl_Human.txt',\n",
    "                   sep='\\t', usecols=['Ensembl ID','UTR sequence']).set_index('Ensembl ID')\n",
    "UTRS['UTR sequence'] = [x.replace('-','').upper().replace('T','U') for x in UTRS['UTR sequence']]\n",
    "# UTRS['UTR length'] = [len(x) for x in UTRS['UTR sequence']]\n",
    "UTRS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7638\n",
      "5854\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UCGUAGG</th>\n",
       "      <th>UCAUCUC</th>\n",
       "      <th>UGCUCUU</th>\n",
       "      <th>UUUGGAA</th>\n",
       "      <th>UUGGAAC</th>\n",
       "      <th>CAAACAC</th>\n",
       "      <th>AAUACAC</th>\n",
       "      <th>UUUCCUC</th>\n",
       "      <th>AGCUUCC</th>\n",
       "      <th>AGUCAGA</th>\n",
       "      <th>...</th>\n",
       "      <th>AAGGCAC</th>\n",
       "      <th>Gene symbol</th>\n",
       "      <th>AGCAGCA</th>\n",
       "      <th>AAAGUGC</th>\n",
       "      <th>AACACUG</th>\n",
       "      <th>AAUACUG</th>\n",
       "      <th>UGACCUA</th>\n",
       "      <th>GAGGUAG</th>\n",
       "      <th>GCAGCAU</th>\n",
       "      <th>Isoform ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ENST00000000412.3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>M6PR</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.078</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001008.4</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.557</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.241</td>\n",
       "      <td>-0.231</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.160</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>FKBP4</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000001146.2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CYP26B1</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.211</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.275</td>\n",
       "      <td>-0.336</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000002165.6</th>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.038</td>\n",
       "      <td>-0.191</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FUCA2</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.999025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENST00000002596.5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HS3ST1</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.277</td>\n",
       "      <td>0.291</td>\n",
       "      <td>-0.420</td>\n",
       "      <td>-0.197</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.144</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   UCGUAGG  UCAUCUC  UGCUCUU  UUUGGAA  UUGGAAC  CAAACAC  \\\n",
       "ENST00000000412.3      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000001008.4   -0.016   -0.557    0.110    0.241   -0.231    0.054   \n",
       "ENST00000001146.2      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "ENST00000002165.6   -0.058    0.092    0.036    0.038   -0.191    0.067   \n",
       "ENST00000002596.5      NaN      NaN      NaN      NaN      NaN      NaN   \n",
       "\n",
       "                   AAUACAC  UUUCCUC  AGCUUCC  AGUCAGA      ...        AAGGCAC  \\\n",
       "ENST00000000412.3      NaN      NaN      NaN      NaN      ...          0.000   \n",
       "ENST00000001008.4   -0.031    0.160   -0.021    0.099      ...          0.056   \n",
       "ENST00000001146.2      NaN      NaN      NaN      NaN      ...            NaN   \n",
       "ENST00000002165.6    0.057    0.046   -0.055   -0.075      ...            NaN   \n",
       "ENST00000002596.5      NaN      NaN      NaN      NaN      ...            NaN   \n",
       "\n",
       "                   Gene symbol  AGCAGCA  AAAGUGC  AACACUG  AAUACUG  UGACCUA  \\\n",
       "ENST00000000412.3         M6PR   -0.098   -0.514   -0.056    0.031   -0.003   \n",
       "ENST00000001008.4        FKBP4    0.016    0.133    0.007   -0.114   -0.166   \n",
       "ENST00000001146.2      CYP26B1   -0.080   -0.053   -0.211    0.150    0.021   \n",
       "ENST00000002165.6        FUCA2    0.063    0.024   -0.018    0.178    0.015   \n",
       "ENST00000002596.5       HS3ST1    0.178   -0.277    0.291   -0.420   -0.197   \n",
       "\n",
       "                   GAGGUAG  GCAGCAU  Isoform ratio  \n",
       "ENST00000000412.3    0.039    0.078       1.000000  \n",
       "ENST00000001008.4    0.002    0.038       1.000000  \n",
       "ENST00000001146.2    0.275   -0.336       1.000000  \n",
       "ENST00000002165.6    0.031   -0.176       0.999025  \n",
       "ENST00000002596.5    0.282    0.144       1.000000  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logFCs1 = pd.read_csv(LOGFC_FILE1)\n",
    "logFCs1 = logFCs1[logFCs1['Used in training'] == 'yes']\n",
    "logFCs1['Transcript ID'] = list(GENE_INFO.loc[logFCs1['Gene symbol']]['Transcript ID'])\n",
    "logFCs1 = logFCs1.drop(['Used in training','RefSeq ID','Gene symbol'],1).dropna(subset=['Transcript ID'])\n",
    "logFCs1 = logFCs1.drop_duplicates(subset=['Transcript ID']).set_index('Transcript ID')\n",
    "logFCs1.columns = [SEED_DICT[x] if x in SEED_DICT else x for x in logFCs1.columns]\n",
    "\n",
    "logFCs2 = pd.read_csv(LOGFC_FILE2)\n",
    "logFCs2['Transcript ID'] = list(GENE_INFO.loc[logFCs2['Gene symbol']]['Transcript ID'])\n",
    "logFCs2 = logFCs2.drop(['RefSeq ID'],1).dropna(subset=['Transcript ID'])\n",
    "logFCs2 = logFCs2.drop_duplicates(subset=['Transcript ID']).set_index('Transcript ID')\n",
    "logFCs2.columns = [SEED_DICT[x] if x in SEED_DICT else x for x in logFCs2.columns]\n",
    "\n",
    "logFCs = pd.concat([logFCs1,logFCs2],axis=1,join='outer').dropna(subset=['Gene symbol'])\n",
    "logFCs['Isoform ratio'] = list(GENE_INFO.loc[logFCs['Gene symbol']]['Isoform ratio'])\n",
    "print(len(logFCs))\n",
    "logFCs = logFCs[logFCs['Isoform ratio'] > 0.9]\n",
    "print(len(logFCs))\n",
    "logFCs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zipped = list(zip(SEEDS, [rev_comp(seed[:-1]) for seed in SEEDS], [SEQ_INFO.loc[seed]['mirna_seq'] for seed in SEEDS]))\n",
    "features, labels, matched, sequences = [], [], [], []\n",
    "\n",
    "for row in logFCs.iterrows():\n",
    "    utr = UTRS.loc[row[0]]['UTR sequence']\n",
    "    row = row[1]\n",
    "    for seed, rev, mirna in zipped:\n",
    "        if len(mirna) < 20:\n",
    "            continue\n",
    "        val = row[seed]\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "        elif utr.count(rev) == 1:\n",
    "            loc = utr.find(rev)\n",
    "            if (loc-15) >= 0:\n",
    "                if (loc + 9) < len(utr):\n",
    "                    seq1, seq2 = mirna[:20], utr[loc-15:loc+9][::-1]\n",
    "                    sequences.append((seq1, seq2))\n",
    "                    features.append(make_square(seq1, seq2))\n",
    "                    labels.append(val)\n",
    "                    matched.append(1)\n",
    "        elif (utr.count(rev) == 0) & (np.random.random() < 0.3):\n",
    "            loc = np.random.randint(0,len(utr))\n",
    "            if (loc-15) >= 0:\n",
    "                if (loc + 9) < len(utr):\n",
    "                    seq1, seq2 = mirna[:20], utr[loc-15:loc+9][::-1]\n",
    "                    sequences.append((seq1, seq2))\n",
    "                    features.append(make_square(seq1, seq2))\n",
    "                    labels.append(val)\n",
    "                    matched.append(0)\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels).reshape(len(labels), 1)\n",
    "test_size = int(len(features)/10)\n",
    "logfc_train = Dataset(features[test_size:], labels[test_size:])\n",
    "logfc_test = Dataset(features[:test_size], labels[:test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_size = 80\n",
    "num_output1 = 8\n",
    "num_output2 = 16\n",
    "fully_connected_nodes = 512\n",
    "out_nodes = 1\n",
    "\n",
    "# create placeholders for data\n",
    "x = tf.placeholder(tf.float32, shape=[None, 20*24*16])\n",
    "x_image = tf.reshape(x, [-1,20*4,24*4,1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, out_nodes])\n",
    "\n",
    "# add convolution that traverses every 4x4 square, without overlaps\n",
    "W_conv1 = weight_variable([4, 4, 1, num_output1])\n",
    "b_conv1 = bias_variable([num_output1])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d_4step(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# add a second convolution that looks at every 2x2 square, with overlaps\n",
    "W_conv2 = weight_variable([2, 2, num_output1, num_output2])\n",
    "b_conv2 = bias_variable([num_output2])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# add a fully connected layer\n",
    "W_fc1 = weight_variable([5 * 6 * num_output2, fully_connected_nodes])\n",
    "b_fc1 = bias_variable([fully_connected_nodes])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 5 * 6 * num_output2])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# add dropout to reduce overfitting\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "W_fc2 = weight_variable([fully_connected_nodes, out_nodes])\n",
    "b_fc2 = bias_variable([out_nodes])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use an L2 loss and Adam optimizer\n",
    "SS_err = tf.reduce_sum(tf.square(tf.sub(y_conv, y_)))\n",
    "SS_tot = tf.reduce_sum(tf.square(tf.sub(y_, tf.reduce_mean(y_))))\n",
    "R_2 = tf.sub(tf.cast(1.0, tf.float32), tf.div(SS_err, SS_tot))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(SS_err)\n",
    "\n",
    "train_model(sess, train_step, R_2, 20000, 50, logfc_train, logfc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
